{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Name: Abd-el-rahman Zaglool\n",
    "\n",
    "###### Project: News classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the project, I'll collect the news from The New York Times webpage. Articles will be retrieved from 8 different sections:\n",
    "1. Business\n",
    "2. Science\n",
    "3. Health\n",
    "4. Sports\n",
    "5. Arts\n",
    "6. Style \n",
    "7. Food\n",
    "8. Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New York Times offer a wide range of handy APIs. One of them is 'Times Wire API', which allows to retrieve the real-time feed of NYT published articles.\n",
    "\n",
    "To fetch articles from the 'Times Wire API' I'll create a helper function named scrape_articles(). \n",
    "\n",
    "The function will take as inputs the desired news' section and the number of articles to retrieve ('limit' parameter).\n",
    " \n",
    "The function returns the response as a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(section, limit):\n",
    "  requestUrl = requestUrl = 'https://api.nytimes.com/svc/news/v3/content/all/'+section+'.json?limit='+str(limit)+'&api-key=X5IOvJkmFhSRsE7jrZHfMM25hEATDJpS'\n",
    "  requestHeaders = {\n",
    "    \"Accept\": \"application/json\"\n",
    "  }\n",
    "\n",
    "  response = requests.get(requestUrl, headers=requestHeaders).json()\n",
    "\n",
    "  return response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also define a second helper function in order to extract the necessary fields ('title', 'abstract' and 'section' of the article) from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_articles(response):\n",
    "        articles = []\n",
    "        docs = response['results']\n",
    "        for doc in docs:\n",
    "                filteredDoc = {}\n",
    "                filteredDoc['title'] = doc['title']\n",
    "                filteredDoc['abstract'] = doc['abstract']\n",
    "                filteredDoc['section'] = doc['section']\n",
    "                articles.append(filteredDoc)\n",
    "\n",
    "        return articles        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll use the two functions defined above to iterate articles retrieval and fields extraction over all of the 8 sections.\n",
    "\n",
    "The result will be a list of lists (8) where each list contains articles from one section only. \n",
    "\n",
    "The articles are stored as dictionaries with three keys: 'title', 'abstract' and 'section'.\n",
    "\n",
    "From each section I've collected at most 500 articles. Number of articles collected may change among sections depending on the feed at the time of code execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = ['business', 'science', 'health', 'sports', 'arts', 'style', 'food', 'travel']\n",
    "limit = 500\n",
    "news = []\n",
    "\n",
    "for section in sections:\n",
    "\n",
    "    response = scrape_articles(section, limit)\n",
    "    section_articles = retrieve_articles(response)\n",
    "    news.append(section_articles)\n",
    "    time.sleep(12)    \n",
    "\n",
    "# time elapsed 1m 57s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the retrieved news in a DataFrame \n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each list of dictionaries\n",
    "for sublist in news:\n",
    "    # Create a temporary DataFrame for each list of dictionaries\n",
    "    temp_df = pd.DataFrame(sublist)\n",
    "    \n",
    "    # Concatenate the temporary DataFrame with the main DataFrame\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "df.to_excel('news.xlsx')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. and Chinese Officials Meet, Businesses...</td>\n",
       "      <td>Chief executives in the U.S. have long pushed ...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investor and His Hedge Fund Are Rocked by Sex ...</td>\n",
       "      <td>After accusations were published in Britain, C...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Men Charged in Case That Spotlights Attacks ...</td>\n",
       "      <td>The homes of New Hampshire Public Radio journa...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two Former Tucker Carlson Producers Exit Fox News</td>\n",
       "      <td>The departures are the latest fallout since th...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitch Star Signs $100 Million Deal With Rival...</td>\n",
       "      <td>The deal signed by Félix Lengyel, known as xQc...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>Beneath a Blanket of Stars</td>\n",
       "      <td>It isn’t as easy as it once was to find a dazz...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>Six Days Afloat in the Everglades</td>\n",
       "      <td>After a storm disrupted plans for a 99-mile pa...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>‘I Identify as an Angler’: Meet Erica Nelson, ...</td>\n",
       "      <td>She hooks tree branches, slips on rocks, and s...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>Will You ‘Go Big’ in Travel This Year?</td>\n",
       "      <td>We want to know if you’ll be packing your bags...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>Unraveling Booster and Vaccine-Timing Rules fo...</td>\n",
       "      <td>The requirements for entering foreign countrie...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     As U.S. and Chinese Officials Meet, Businesses...   \n",
       "1     Investor and His Hedge Fund Are Rocked by Sex ...   \n",
       "2     3 Men Charged in Case That Spotlights Attacks ...   \n",
       "3     Two Former Tucker Carlson Producers Exit Fox News   \n",
       "4     Twitch Star Signs $100 Million Deal With Rival...   \n",
       "...                                                 ...   \n",
       "3995                         Beneath a Blanket of Stars   \n",
       "3996                  Six Days Afloat in the Everglades   \n",
       "3997  ‘I Identify as an Angler’: Meet Erica Nelson, ...   \n",
       "3998             Will You ‘Go Big’ in Travel This Year?   \n",
       "3999  Unraveling Booster and Vaccine-Timing Rules fo...   \n",
       "\n",
       "                                               abstract   section  \n",
       "0     Chief executives in the U.S. have long pushed ...  Business  \n",
       "1     After accusations were published in Britain, C...  Business  \n",
       "2     The homes of New Hampshire Public Radio journa...  Business  \n",
       "3     The departures are the latest fallout since th...  Business  \n",
       "4     The deal signed by Félix Lengyel, known as xQc...  Business  \n",
       "...                                                 ...       ...  \n",
       "3995  It isn’t as easy as it once was to find a dazz...    Travel  \n",
       "3996  After a storm disrupted plans for a 99-mile pa...    Travel  \n",
       "3997  She hooks tree branches, slips on rocks, and s...    Travel  \n",
       "3998  We want to know if you’ll be packing your bags...    Travel  \n",
       "3999  The requirements for entering foreign countrie...    Travel  \n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "section\n",
       "Business    500\n",
       "Science     500\n",
       "Health      500\n",
       "Sports      500\n",
       "Arts        500\n",
       "Style       500\n",
       "Food        500\n",
       "Travel      500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualizing number of articles retrieved by section\n",
    "df.section.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with NaN on both title and abstract:\n",
    "df = df.drop(index= df[(df['title'].isna()) & df['abstract'].isna()].index)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. and Chinese Officials Meet, Businesses...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investor and His Hedge Fund Are Rocked by Sex ...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Men Charged in Case That Spotlights Attacks ...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two Former Tucker Carlson Producers Exit Fox N...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitch Star Signs $100 Million Deal With Rival...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>Beneath a Blanket of Stars : It isn’t as easy ...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>Six Days Afloat in the Everglades : After a st...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>‘I Identify as an Angler’: Meet Erica Nelson, ...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>Will You ‘Go Big’ in Travel This Year? : We wa...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3991</th>\n",
       "      <td>Unraveling Booster and Vaccine-Timing Rules fo...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3992 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article   section\n",
       "0     As U.S. and Chinese Officials Meet, Businesses...  Business\n",
       "1     Investor and His Hedge Fund Are Rocked by Sex ...  Business\n",
       "2     3 Men Charged in Case That Spotlights Attacks ...  Business\n",
       "3     Two Former Tucker Carlson Producers Exit Fox N...  Business\n",
       "4     Twitch Star Signs $100 Million Deal With Rival...  Business\n",
       "...                                                 ...       ...\n",
       "3987  Beneath a Blanket of Stars : It isn’t as easy ...    Travel\n",
       "3988  Six Days Afloat in the Everglades : After a st...    Travel\n",
       "3989  ‘I Identify as an Angler’: Meet Erica Nelson, ...    Travel\n",
       "3990  Will You ‘Go Big’ in Travel This Year? : We wa...    Travel\n",
       "3991  Unraveling Booster and Vaccine-Timing Rules fo...    Travel\n",
       "\n",
       "[3992 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenating title and abstract into the same 'article' column\n",
    "# Some rows have NA in title, others have NA in abstract so I have to handle the different cases \n",
    "\n",
    "df.loc[df[df['abstract'].isna()].index, 'title'] = df[df['abstract'].isna()]['title'].values #NA in abstract \n",
    "df.loc[df[(df['title'].notnull()) & (df['abstract'].notnull())].index, 'title'] = df[(df['title'].notnull()) & (df['abstract'].notnull())]['title'].values + ' : ' + df[(df['title'].notnull()) & (df['abstract'].notnull())]['abstract'].values #no NAs\n",
    "df.loc[df[df['title'].isna()].index, 'title'] = df[df['title'].isna()]['abstract'].values #NA in title\n",
    "df.rename(columns={'title' : 'article'}, inplace=True)\n",
    "df.drop(columns= df.columns[1], axis=1, inplace=True)\n",
    "df    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll set up the necessary components for text preprocessing tasks. \n",
    "\n",
    "Preprocessing involve text tokenization, stop words removal, part-of-speech mapping and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #retrieving english stop words list\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #initializing tokenizer\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Maps the input word's POS tag to the appropriate WordNet POS tag\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() #initializing lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a for loop to preprocess each article of the news dataframe\n",
    "\n",
    "nrows= df.shape[0]\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, nrows):\n",
    "    globals()['article%s' % i] = df.iloc[i,0]\n",
    "\n",
    "    globals()['word_tokens%s' % i] = tokenizer.tokenize(globals()['article%s' % i]) #tokenization by word\n",
    "    globals()['filtered_sentence%s' % i] = [w.lower() for w in globals()['word_tokens%s' % i] if not w.lower() in stop_words]  # insert only words that aren't present in stop_words. In lowercase\n",
    "\n",
    "    globals()['filtered_sentence_lemmatized%s' % i] = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in globals()['filtered_sentence%s' % i]] #lemmatize the worlds in globals()['filtered_sentence%s' % i]\n",
    "    globals()['filtered_sentence_lemmatized%s' % i] = ' '.join(globals()['filtered_sentence_lemmatized%s' % i])\n",
    "    corpus.append(globals()['filtered_sentence_lemmatized%s' % i])\n",
    "\n",
    "# time elapsed: 5m 14s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll save then the processed articles as a new column in my DataFrame.\n",
    "\n",
    "In addition, I'll factorize the sections (i.e. assign a different number to each section). 'Factorized_section' column will be my target variable in the news classification problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>section</th>\n",
       "      <th>processed_article</th>\n",
       "      <th>factorized_section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. and Chinese Officials Meet, Businesses...</td>\n",
       "      <td>Business</td>\n",
       "      <td>u chinese official meet business temper hope c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investor and His Hedge Fund Are Rocked by Sex ...</td>\n",
       "      <td>Business</td>\n",
       "      <td>investor hedge fund rock sex assault allegatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Men Charged in Case That Spotlights Attacks ...</td>\n",
       "      <td>Business</td>\n",
       "      <td>3 men charge case spotlight attack medium home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two Former Tucker Carlson Producers Exit Fox N...</td>\n",
       "      <td>Business</td>\n",
       "      <td>two former tucker carlson producer exit fox ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitch Star Signs $100 Million Deal With Rival...</td>\n",
       "      <td>Business</td>\n",
       "      <td>twitch star sign 100 million deal rival platfo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>Beneath a Blanket of Stars : It isn’t as easy ...</td>\n",
       "      <td>Travel</td>\n",
       "      <td>beneath blanket star easy find dazzle night sk...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3987</th>\n",
       "      <td>Six Days Afloat in the Everglades : After a st...</td>\n",
       "      <td>Travel</td>\n",
       "      <td>six day afloat everglades storm disrupt plan 9...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>‘I Identify as an Angler’: Meet Erica Nelson, ...</td>\n",
       "      <td>Travel</td>\n",
       "      <td>identify angler meet erica nelson female indig...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>Will You ‘Go Big’ in Travel This Year? : We wa...</td>\n",
       "      <td>Travel</td>\n",
       "      <td>go big travel year want know pack bag 2022</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>Unraveling Booster and Vaccine-Timing Rules fo...</td>\n",
       "      <td>Travel</td>\n",
       "      <td>unravel booster vaccine timing rule internatio...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3991 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article   section  \\\n",
       "0     As U.S. and Chinese Officials Meet, Businesses...  Business   \n",
       "1     Investor and His Hedge Fund Are Rocked by Sex ...  Business   \n",
       "2     3 Men Charged in Case That Spotlights Attacks ...  Business   \n",
       "3     Two Former Tucker Carlson Producers Exit Fox N...  Business   \n",
       "4     Twitch Star Signs $100 Million Deal With Rival...  Business   \n",
       "...                                                 ...       ...   \n",
       "3986  Beneath a Blanket of Stars : It isn’t as easy ...    Travel   \n",
       "3987  Six Days Afloat in the Everglades : After a st...    Travel   \n",
       "3988  ‘I Identify as an Angler’: Meet Erica Nelson, ...    Travel   \n",
       "3989  Will You ‘Go Big’ in Travel This Year? : We wa...    Travel   \n",
       "3990  Unraveling Booster and Vaccine-Timing Rules fo...    Travel   \n",
       "\n",
       "                                      processed_article  factorized_section  \n",
       "0     u chinese official meet business temper hope c...                   0  \n",
       "1     investor hedge fund rock sex assault allegatio...                   0  \n",
       "2     3 men charge case spotlight attack medium home...                   0  \n",
       "3     two former tucker carlson producer exit fox ne...                   0  \n",
       "4     twitch star sign 100 million deal rival platfo...                   0  \n",
       "...                                                 ...                 ...  \n",
       "3986  beneath blanket star easy find dazzle night sk...                   7  \n",
       "3987  six day afloat everglades storm disrupt plan 9...                   7  \n",
       "3988  identify angler meet erica nelson female indig...                   7  \n",
       "3989         go big travel year want know pack bag 2022                   7  \n",
       "3990  unravel booster vaccine timing rule internatio...                   7  \n",
       "\n",
       "[3991 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_article'] = corpus\n",
    "df['factorized_section'] = df['section'].factorize()[0]\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>factorized_section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Health</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>Sports</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>Arts</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>Style</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>Food</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492</th>\n",
       "      <td>Travel</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       section  factorized_section\n",
       "0     Business                   0\n",
       "500    Science                   1\n",
       "998     Health                   2\n",
       "1498    Sports                   3\n",
       "1992      Arts                   4\n",
       "2492     Style                   5\n",
       "2992      Food                   6\n",
       "3492    Travel                   7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the factorization legend\n",
    "df[['section', 'factorized_section']].drop_duplicates().sort_values('factorized_section')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be fed to the classification model, the processed articles must be converted into a numerical representation. In particular, with  the help Scikit-learn's CountVectorizer class I'll represent the articles as a sparse matrix with\n",
    "- n rows = n processed articles \n",
    "- n columns = n unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,2].values\n",
    "y = df.factorized_section.values\n",
    "\n",
    "# Init CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "# Fit to the corpus\n",
    "x = cv.fit_transform(x).toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I only need to split the data in two portions (train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test portion is 80% of data, test is the remaining 20%\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm ready to call the classification model.\n",
    "\n",
    "To classify the articles I chose a basic Logistic Regression since it offers a good trade-off between performance and complexity (i.e. computational expense). \n",
    "\n",
    "Given that news classification problem involves multiple labels, I'll resort to OneVsRestClassifier() class. OneVsRest takes a binary classifier (the Logistic Regression) and wraps it to handle multiclass classification problems by decomposing them into multiple binary classification tasks.\n",
    "\n",
    "To evaluate the predictive power of the model, considering the equal importance of all labels, I relied on the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Basic Logistic Regression: 79.6 %\n"
     ]
    }
   ],
   "source": [
    "mdl = LogisticRegression(random_state=42)\n",
    "\n",
    "oneVsRest = OneVsRestClassifier(mdl)\n",
    "\n",
    "oneVsRest.fit(x_train, y_train)\n",
    "\n",
    "y_pred = oneVsRest.predict(x_test)\n",
    "\n",
    "# Performance\n",
    "accuracy = round(accuracy_score(y_test, y_pred)*100, 2)\n",
    "print(f'Accuracy Score of Basic Logistic Regression: {accuracy} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy score measures the proportion of correctly predicted labels compared to the total number of samples. On average, our Logistic regression correctly predicts the section of 8 articles out of 10! That's already a great result, which can be further enhanced by, for example, employing some hyperparameter fine-tuning..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
